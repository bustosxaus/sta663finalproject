{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STA 663 Final Project: “The No-U-Turn Sampler”  \n",
    "\n",
    "# Sarah Normoyle, Gonzalo Bustos  \n",
    "# April 27, 2016  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many models, Monte Carlo Markov Chain (MCMC) methods such as Gibbs sampling and the Metropolis Hasting algorithm may not be efficient and may require a long time to converge. By using steps that are evaluated from the first-order gradient of the log posterior, Hamiltonian Monte Carlo (HMC) is an efficient MCMC algorithm that does not use random walk behavior. This paper by Matthew D. Hoffman and Andrew Gelman introduces a new algorithm, called the No-U-Turn Sampler (NUTS) that is an extension of Hamiltonian Monte Carlo. Unlike HMC, NUTS does not require the specification of the parameter for the number of steps, L. In addition, the use of a dual averaging technique is extended from HMC to NUTS in order to avoid the specification of a step size parameter, $\\epsilon$. Therefore, unlike HMC, NUTS can be implemented without having to hand-tune both of the two parameters, L and $\\epsilon$. In our report, we will implement the Naive NUTS Algorithm and also extend to the NUTS Algorithm with Dual Averaging. We will compare the efficiency and the results of this algorithm to other MCMC algorithms in Stan when used for a specific model and data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Hamiltonian Monte Carlo (HMC) depends strongly on choosing suitable values for $\\varepsilon$ and $L$, which is the number of times chosen to run the leapfrog step. If $\\varepsilon$ is too large, then the simulation will be inaccurate and yield low acceptance rates. If $\\varepsilon$ is too small, then computation will be wasted taking many small steps. If $L$ is too small, then successive samples will be close to one another, resulting in undesirable random walk behavior and slow mixing. If $L$ is too large, then HMC will generate trajectories that loop back and retrace their steps.\n",
    "\n",
    "The No-U-Turn Sampler (NUTS) is an extension of HMC that eliminates the need to specify a fixed value of $L$, the number of leapfrog steps. It also incorporates schemes for setting $\\varepsilon$ based on a dual averaging algorithm.\n",
    "\n",
    "Algorithm 1 from the paper goes through the Leapfrog function, which is also used in HMC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Leapfrog step** from **Algorithm 1**\n",
    "\n",
    "**function** Leapfrog$(\\theta,r,\\varepsilon)$\n",
    "\n",
    "Set $\\tilde{r} \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta)$\n",
    "\n",
    "Set $\\tilde{\\theta} \\leftarrow \\theta + \\varepsilon \\tilde{r}$\n",
    "\n",
    "Set $\\tilde{r} \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\tilde{\\theta})$\n",
    "\n",
    "**return** $\\tilde{\\theta},\\tilde{r}$\n",
    "\n",
    "$\\mathcal{L}$ is the logarithm of the joint density of the variables of interest $\\theta$. The Leapfrog function of Algorithm 1 implements the Stormer-Verlet (\"leapfrog\") integrator, which proceeds according to the updates:\n",
    "\n",
    "$r^{t + \\frac{\\varepsilon}{2}} = r^t + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta^t)$\n",
    "\n",
    "$\\theta^{t + \\varepsilon} = \\theta^t + \\varepsilon r^{t + \\frac{\\varepsilon}{2}}$\n",
    "\n",
    "$r^{t + \\varepsilon} = r^{t + \\frac{\\varepsilon}{2}} + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta^{t + \\varepsilon})$\n",
    "\n",
    "where $r^t$ and $\\theta^t$ denote the values of the momentum and position variables $r$ and $\\theta$ at time $t$, $\\nabla_\\theta$ denotes the gradient with respect to $\\theta$ and $\\varepsilon$ is the step size parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing Algorithm 3, Efficient NUTS, the paper develops Algorithm 2, Naive NUTS. Algorithm 2 introduces a slice variable $u$ with conditional distribution $p(u|\\theta,r)=\\text{Uniform}(u;[0,\\text{exp}\\{\\mathcal{L}(\\theta) - \\frac{1}{2} r \\cdot r \\}])$, which renders the conditional distribution $p(\\theta,r|u) = \\text{Uniform} (\\theta,r;\\{\\theta',r'|\\mathcal{L}(\\theta) - \\frac{1}{2} r \\cdot r \\} \\geq u \\})$. After resampling $u|\\theta,r$, NUTS uses the leapfrog algorithm to trace out a path forwards and backwards, doing for 1 step, 2 steps, 4 steps, etc. This doubling process builds a balanced binary tree whose leaf-nodes correspond to position-momentum states. The process is halted when the trajectory starts to double back on itself. \n",
    "\n",
    "In summary, Algorithm 2 leaves the target distribution $p(\\theta) \\propto \\text{exp}\\{\\mathcal{L}(\\theta)\\}$ invariant. It achieves this by resampling the momentum and slice variables $r$ and $u$, simulating a Hamiltonian trajectory forwards and backwards in time until that trajectory either begins retracing its steps or encounters a state with very low probability, selecting a subset of the states encountered on that trajectory that lie within the slice defined by the slice variable $u$, and finally choosing the next position and momentum variables $\\theta^m$ and $r$ uniformly at random from the subset of the states encountered.\n",
    "\n",
    "Algorithm 3 improves Algorithm 2 by breaking out of the recursion as soon as a zero value for the stop indicator $s$ is encountered.\n",
    "\n",
    "Below is the first implementation of the NUTS sampler. It uses the Leapfrog function used in Hamiltonian Monte Carlo as well as a new BuildTree function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 3** Efficient NUTS\n",
    "\n",
    "Given $\\theta^0, \\varepsilon, \\mathcal{L}, M$:\n",
    "\n",
    "**for** $m=1$ to $M$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $r^0 \\sim \\mathcal{N}(0,I)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $u \\sim \\text{Uniform}([0, \\text{exp}\\{\\mathcal{L}(\\theta^{m-1}) - \\frac{1}{2} r^0 \\cdot r^0 \\}])$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize $\\theta^- = \\theta^{m-1},~\\theta^+ = \\theta^{m-1},~r^- = r^0,~r^+ = r^0,~j = 0,~\\theta^m=\\theta^{m-1},~n=1,~s=1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **while** $s=1$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choose a direction $v_j \\sim \\text{Uniform}(\\{-1,1\\})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v_j=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v_j,j,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v_j,j,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\text{min}\\big\\{1,\\frac{n'}{n}\\big\\}$, set $\\theta^m \\leftarrow \\theta'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n \\leftarrow n + n'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s \\leftarrow s' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $j \\leftarrow j+1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end while**\n",
    "\n",
    "**end for**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**function** BuildTree$(\\theta,r,u,v,j,\\varepsilon)$\n",
    "\n",
    "**if** $j=0$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Base case - take one leapfrog step in the direction $v$*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta',r' \\leftarrow \\text{Leapfrog}(\\theta,r,v\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow \\mathbb{1}[u \\leq \\text{exp}\\{\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' \\}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow \\mathbb{1}[\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' > \\text{log}~u - \\Delta_{max}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta',r',\\theta',r',\\theta',n',s'$\n",
    "\n",
    "**else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Recursion - implicitly build the left and right subtrees*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta,r,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta'',n'',s'' \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta'',n'',s'' \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\frac{n''}{n'+n''}$, set $\\theta' \\leftarrow \\theta''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow s'' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow n' + n''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s'$\n",
    "\n",
    "**end if**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 6 further improves NUTS by adaptively tuning $\\varepsilon$. In order to do this the paper uses the stichastic approximation method, using a statistic $H_t$ that describes some aspect of the behavior of an MCMC algorithm at iteration $t \\geq 1$. The expectation of $H_t$ is defined as:\n",
    "\n",
    "$h(x) = \\mathbb{E}[H_t|x] = \\lim_{T\\to\\infty}\\frac{1}{T} \\sum_{t=1}^T \\mathbb{E}[H_t|x]$\n",
    "\n",
    "$x \\in \\mathbb{R}$ is a tunable parameter to the MCMC algorithm. If $h$ is a non-decreasing function of $x$, the update $x_{t+1} \\leftarrow x_t - \\eta_t H_t$ is guaranteed to cause $h(x_t)$ to converge to 0, as long as the step size $\\eta_t$ satisfies the following:\n",
    "\n",
    "$\\sum_t \\eta_t = \\infty$\n",
    "\n",
    "$\\sum_t \\eta^2_t < \\infty$\n",
    "\n",
    "These conditions are satisfied for $\\eta_t = t^{-\\kappa}$, for $\\kappa \\in (0.5,1]$.\n",
    "\n",
    "Adapting the dual averaging scheme, an algorithm for nonsmooth and stochastic convex optimization, to the problem of MCMC adaptation by replacing stochastic gradients with the statistic $H_t$, the updates are the following:\n",
    "\n",
    "(i)$~~x_{t+1} \\leftarrow \\mu - \\frac{\\sqrt{t}}{\\gamma} \\frac{1}{t+t_0} \\sum_{i=1}^t H_i$\n",
    "\n",
    "(ii)$~~\\bar{x}_{t+1} \\leftarrow \\eta_t x_{t+1} + (1 - \\eta_t) \\bar{x}_t$\n",
    "\n",
    "Where $\\mu$ is a chosen point to where the iterates $x_t$ are shrunk towards, $\\gamma > 0$ is a parameter that controls the amount of shrinkage towards $\\mu$, $t_0 \\geq 0$ is a parameter that stabilizes the initial iterations and $\\bar{x}_1 = x_1$. The paper uses te values $\\gamma=0.05$, $t_0=10$ and $\\kappa=0.75$, obtained by trying settings by hand on a model used as an example.\n",
    "\n",
    "Eventhough, the dual averaging scheme should work for any initial value $\\varepsilon_1$ and any shrinkage target $\\mu$, the paper recommends choosing an initial value $\\varepsilon_1$ according to the heuristic in Algorithm 4, which results in a value that is small enough to produce reasonably accurate simulations but large enough to avoid wasting large amounts of computation. The paper also recommends setting $\\mu=log(10\\varepsilon_1)$, in order to use larger values of $\\varepsilon$ and save computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 4** Heuristic for choosing an initial values of $\\varepsilon$\n",
    "\n",
    "**function** FindReasonableEpsilon($\\theta$)\n",
    "\n",
    "Initialize $\\varepsilon =1, r \\sim \\mathcal{N}(0,I)$  \n",
    "\n",
    "Set $\\theta', r' \\leftarrow \\text{Leapfrog}(\\theta,r,\\varepsilon)$\n",
    "\n",
    "$a \\leftarrow 2 \\mathbb{1} \\Big[\\frac{p(\\theta',r')}{p(\\theta,r)} > 0.5\\Big]  - 1$\n",
    "\n",
    "**while** $\\Big(\\frac{p(\\theta',r')}{p(\\theta,r)}\\Big)^a > 2^{-a}$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\varepsilon \\leftarrow 2^a \\varepsilon$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Set $\\theta', r' \\leftarrow \\text{Leapfrog}(\\theta,r,\\varepsilon)$\n",
    "\n",
    "**end while**\n",
    "\n",
    "**return** $\\varepsilon$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to apply the dual averaging scheme for $\\varepsilon$ it is necessary to define, for each iteration, the following statistic:\n",
    "\n",
    "$H_t^{NUTS} = \\frac{1}{|\\mathcal{B}_t^{final}|} \\sum_{\\theta,r \\in \\mathcal{B}_t^{final}} \\text{min}\\Big\\{1,\\frac{p(\\theta,r)}{p(\\theta^{t-1},r^{t,0})} \\Big\\}$\n",
    "\n",
    "$h^{NUTS} = \\mathbb{E} [H_t^{NUTS}]$\n",
    "\n",
    "$\\mathcal{B}_t^{final}$ is the set of all the states explored during the final iteration $t$ of the Markov chain and $\\theta^{t-1}$ and $r^{t,0}$ are the initial position and momentum for the $t^{th}$ iteration of the Markov chain. Assuming that $H^{NUTS}$ is non-increasing in $\\varepsilon$, equations (i) and (ii) can be used with $H_t = \\delta - H^{NUTS}$ and $x = \\text{log}~\\varepsilon$ to coerce $h^{NUTS} = \\delta$ for any $\\delta \\in (0,1)$.\n",
    "\n",
    "Algorithm 6 implements NUTS while incorporating the dual averaging algorithm, requiring a target mean acceptance probability $\\delta$ and a number of iterations $M^{adapt}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 6** NUTS with Dual Averaging\n",
    "\n",
    "Given $\\theta^0, \\delta, \\mathcal{L}, M, M^{adapt}$:\n",
    "\n",
    "Set $\\varepsilon_0=\\text{FindReasonableEpsilon}(\\theta), \\mu=\\text{log}(10\\varepsilon_0), \\bar{\\varepsilon}_0=1, \\bar{H}=0, \\gamma=0.05, t_0=10,\\kappa=0.75$\n",
    "\n",
    "**for** $m=1$ to $M$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Sample $r^0 \\sim \\mathcal{N}(0,I)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $u \\sim \\text{Uniform}([0, \\text{exp}\\{\\mathcal{L}(\\theta^{m-1}) - \\frac{1}{2} r^0 \\cdot r^0 \\}])$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize $\\theta^- = \\theta^{m-1},~\\theta^+ = \\theta^{m-1},~r^- = r^0,~r^+ = r^0,~j = 0,~\\theta^m=\\theta^{m-1},~n=1,~s=1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **while** $s=1$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choose a direction $v_j \\sim \\text{Uniform}(\\{-1,1\\})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v_j=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta',n',s',\\alpha,n_\\alpha \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v_j,j,\\varepsilon_{m-1},\\theta^{m-1},r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta',n',s',\\alpha,n_\\alpha \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v_j,j,\\varepsilon_{m-1},\\theta^{m-1},r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\text{min}\\big\\{1,\\frac{n'}{n}\\big\\}$, set $\\theta^m \\leftarrow \\theta'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n \\leftarrow n + n'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s \\leftarrow s' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $j \\leftarrow j+1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end while**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **if** $m \\leq M^{adapt}$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $\\bar{H}_m = \\Big(1 - \\frac{1}{m+t_0} \\Big) \\bar{H}_{m-1} + \\frac{1}{m+t_0} \\Big( \\delta + - \\frac{\\alpha}{n_\\alpha} \\Big)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $\\text{log}~\\varepsilon_m = \\mu - \\frac{\\sqrt{m}}{\\gamma} \\bar{H}_m, \\text{log}~\\bar{\\epsilon}_m = m^{-\\kappa}~\\text{log}~\\varepsilon_m + (1 - m^{-\\kappa})~\\text{log}~\\bar{\\varepsilon}_{m-1}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $\\varepsilon_m = \\bar{\\varepsilon}_{M^{adapt}}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "**end for**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**function** BuildTree$(\\theta,r,u,v,j,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "**if** $j=0$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Base case - take one leapfrog step in the direction $v$*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta',r' \\leftarrow \\text{Leapfrog}(\\theta,r,v\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow \\mathbb{1}[u \\leq \\text{exp}\\{\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' \\}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow \\mathbb{1}[\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' > \\text{log}~u - \\Delta_{max}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta',r',\\theta',r',\\theta',n',s',\\text{min}\\{1,exp\\{\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' - \\mathcal{L}(\\theta^0) + \\frac{1}{2} r^0 \\cdot r^0 \\} \\}, 1$ \n",
    "\n",
    "**else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Recursion - implicitly build the left and right subtrees*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s',\\alpha',n'_\\alpha \\leftarrow \\text{BuildTree}(\\theta,r,u,v,j-1,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta'',n'',s'',\\alpha'',n''_\\alpha \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v,j-1,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta'',n'',s'',\\alpha'',n''_\\alpha \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v,j-1,\\varepsilon,\\theta^0,r^0)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\frac{n''}{n'+n''}$, set $\\theta' \\leftarrow \\theta''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow s'' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow n' + n''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s',\\alpha',n'_\\alpha$\n",
    "\n",
    "**end if**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose to implement Algorithm 6, the NUTS sampler with Dual Averaging adapted from Hamiltonian Monte Carlo because it does not require the specification of a value of epsilon. This algorithm requires 3 additional helper functions, FindReasonableEpsilon, BuildTree, and Leapfrog. Therefore, in total we implemented 4 algorithms.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unoptimized Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first implementation, we followed the pseudocode pretty much exactly as follows above. This requires re-calculating the gradient many times in each of the helper functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Leapfrog_slow(theta, r, eps, L):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    From Algorithm 1 in NUTS, Leapfrog step in Hamiltonian Monte Carlo/NUTS.\n",
    "    Helper function for Algorithm 6. \n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "       \n",
    "    OUTPUTS:\n",
    "    theta_tilde = update theta vector\n",
    "    \n",
    "    r_tilde = updated r momenta vector\n",
    "    \"\"\"\n",
    "    \n",
    "    logp, grad = L(theta)\n",
    "    r_tilde = r + (eps/2) * grad\n",
    "    theta_tilde = theta + eps * r_tilde\n",
    "    logp_tilde, grad_tilde = L(theta_tilde)\n",
    "    r_tilde = r_tilde + (eps/2) * grad_tilde\n",
    "    return theta_tilde, r_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindReasonableEpsilon_slow(theta, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    From Algorithm 4 in NUTS.\n",
    "    Heuristic for choosing an initial value of epsilon.\n",
    "    Helper function for Algorithm 6.\n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    eps = value for initial value of epsilon, step size\n",
    "    \n",
    "    \"\"\"\n",
    "    #initialize\n",
    "    parems = len(theta)\n",
    "    eps = 1\n",
    "    r = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "    r = r.ravel()\n",
    "    theta_prime, r_prime = Leapfrog_slow(theta, r, eps, L)\n",
    "    \n",
    "    logp, grad = L(theta)\n",
    "    logp_prime, grad_prime = L(theta_prime)\n",
    "    \n",
    "    prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r, r)))\n",
    "    a = 2 * int(prob > 0.5) - 1\n",
    "\n",
    "    while prob**a > 2**(-a):\n",
    "        eps = 2**a * eps\n",
    "        theta_prime, r_prime = Leapfrog_slow(theta, r, eps, L);\n",
    "        logp_prime, grad_prime = L(theta_prime)\n",
    "        prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r,r)))\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BuildTree_slow(theta, r, u, v, j, eps, r_theta0, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    builds tree in NUTS sampler, helper function for Algorithm 6\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    v = direction in creating tree, value from -1 to 1\n",
    "    \n",
    "    j = height of tree, starts at 0\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    r_theta0 = joint probability of theta0 and r\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime\n",
    "    \n",
    "    \"\"\"\n",
    "    if j == 0: \n",
    "        # base case, take one leapfrog step in the direction v\n",
    "        theta_prime, r_prime = Leapfrog_slow(theta, r, v*eps, L)\n",
    "        logp_prime, grad_prime = L(theta_prime)\n",
    "        logp_0, grad0 = L(theta0)\n",
    "        \n",
    "        r_theta = logp_prime - 0.5 * np.dot(r_prime, r_prime)      \n",
    "\n",
    "        n_prime = int(u <= np.exp(r_theta))   \n",
    "        s_prime = int(r_theta > np.log(u) - 1000)\n",
    "        alpha_prime = min(1, np.exp(r_theta - r_theta0))\n",
    "                                                   \n",
    "        return theta_prime, r_prime, theta_prime, r_prime, theta_prime, n_prime, s_prime, alpha_prime, 1\n",
    "    else:\n",
    "        # recursion, build left and right subtrees\n",
    "        theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime = BuildTree_slow(theta, r, u, v, j-1, eps, r_theta0, L)\n",
    "        \n",
    "        if s_prime == 1:\n",
    "            if v == -1:\n",
    "                theta_minus, r_minus, _,_, theta_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree_slow(theta_minus, r_minus, u, v, j-1, eps, r_theta0, L)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree_slow(theta_plus, r_plus, u, v, j-1, eps, r_theta0, L)\n",
    "\n",
    "            # Use Metropolis-Hastings\n",
    "            prob = n_doub_prime / max(n_prime + n_doub_prime,1)\n",
    "            if (np.random.uniform(0, 1, 1) < prob):\n",
    "                theta_prime = theta_doub_prime\n",
    "            \n",
    "            ind_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            ind_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s_prime = s_prime * s_doub_prime * ind_1 * ind_2\n",
    "            n_prime = n_prime + n_doub_prime\n",
    "            alpha_prime = alpha_prime + alpha_doub_prime\n",
    "            n_alpha_prime = n_alpha_prime + n_alpha_doub_prime\n",
    "        \n",
    "        return theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nuts6_dual_averaging_slow(theta0, M, M_adapt, L, delta = 0.6):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    Not optimized version. \n",
    "    Implemented from Algorithm 6: NUTS with Dual Averaging.\n",
    "    Needs helper functions BuildTree_slow, FindReasonableEpsilon_slow, and Leapfrog_slow.\n",
    "    \n",
    "\n",
    "    INPUTS:\n",
    "    \n",
    "    theta0 = initial values for values of parameters in model. len(theta0) = number of parameters\n",
    "    \n",
    "    M = number of samples desired\n",
    "    \n",
    "    M_adapt = the number of steps for the burn-in,\n",
    "    also how long to run the dual averaging algorithm to find the appropriate epsilon\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    delta = target HMC acceptance probability.\n",
    "    default value of 0.6\n",
    "    is a value between 0 and 1\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    samples = np.array matrix of samples of theta from algorithm\n",
    "    dimensions of matrix are M x len(theta0)\n",
    "    \n",
    "    burned_in = same as samples matrix with burn_in samples removed\n",
    "    dimensions of matrix are are M-M_adapt x len(theta0)\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize samples matrix\n",
    "    # put initial theta0 in first row of matrix\n",
    "    parems = len(theta0)\n",
    "    samples = np.empty((M+1, parems))\n",
    "    samples[0, :] = theta0\n",
    "    eps = FindReasonableEpsilon_slow(theta0, L)\n",
    "    mu = np.log(10*eps)\n",
    "    eps_bar = 1\n",
    "    H_bar = 0\n",
    "    gamma = 0.05\n",
    "    t0 = 10\n",
    "    k = 0.75\n",
    "    \n",
    "    for m in range(1, M+1):\n",
    "        # resample\n",
    "        norm_samp = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "        r0 = norm_samp.ravel()\n",
    "        logp, grad = L(samples[m-1,:])\n",
    "\n",
    "        r_theta = logp - 0.5 * np.dot(r0,r0)\n",
    "        # resample u ~ uniform([0, exp(inside)])\n",
    "        u = np.random.uniform(0, np.exp(r_theta), 1)\n",
    "\n",
    "        # initialize minus's and plus's\n",
    "        theta_minus = samples[m-1, :]\n",
    "        theta_plus = samples[m-1, :]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j = 0 \n",
    "        \n",
    "        samples[m, :] = samples[m-1, :]\n",
    "        n = 1\n",
    "        s = 1\n",
    "        \n",
    "        while s == 1:\n",
    "            v_j = np.random.uniform(-1,1,1)\n",
    "            if v_j == -1:\n",
    "                theta_minus, r_minus, _, _, theta_prime, n_prime, s_prime, alpha, n_alpha = BuildTree_slow(theta_minus, r_minus, u, v_j, j, eps, r_theta, L)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha, n_alpha = BuildTree_slow(theta_plus, r_plus, u, v_j, j, eps, r_theta, L)\n",
    "            \n",
    "            if s_prime == 1:\n",
    "                # Use Metropolis-Hastings\n",
    "                prob = min(1, n_prime/n)\n",
    "                if (np.random.uniform(0,1,1) < prob):\n",
    "                    samples[m, :] = theta_prime\n",
    "                    \n",
    "            n = n + n_prime\n",
    "\n",
    "            boolean_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            boolean_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s = s_prime * boolean_1 * boolean_2\n",
    "            j = j + 1\n",
    "            \n",
    "        if m <= M_adapt:\n",
    "            H_bar = (1 - 1/(m+t0))*H_bar + (1/(m+t0)) * (delta - alpha/n_alpha)\n",
    "            eps = np.exp(mu - np.sqrt(m)/gamma * H_bar)\n",
    "            eps_bar = np.exp(m**(-k) * np.log(eps) + (1-m**(-k))*np.log(eps_bar))\n",
    "        else:\n",
    "            eps = eps_bar\n",
    "            \n",
    "    burned_in = samples[M_adapt+1:M+1, :]\n",
    "    \n",
    "    return samples, burned_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To optimize the code, we first tried jit from Numba to speed up the code. There was no speed ups determined, and we considered other options. \n",
    "\n",
    "Next, we made sure opportunities for vectorization were implemented, which they were in places that were needed. \n",
    "\n",
    "Next, we noticed that the log likelihood and the gradient had to be re-calculated for values of $\\theta$ and $\\theta'$ when being passed through various functions, such as Leapfrog and BuildTree. The more dimensions of $\\theta$ we have, the longer re-calculating these values takes. Therfore, we can add the values of the log likelihood and the gradient as outputs and inputs for Leapfrom, BuildTree, and FindReasonableEpsilon. If the values of the log likelihood and gradient follows the value of $\\theta$ throughout the algorithm, then we thought algorithm would speed up. We thus implemented new functions for each of the four functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Leapfrog(theta, r, grad, eps, L):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    From Algorithm 1 in NUTS, Leapfrog step in Hamiltonian Monte Carlo/NUTS.\n",
    "    Helper function for Algorithm 6. \n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "       \n",
    "    OUTPUTS:\n",
    "    theta_tilde = update theta vector\n",
    "    \n",
    "    r_tilde = updated r momenta vector\n",
    "    \"\"\"\n",
    "    \n",
    "    r_tilde = r + (eps/2) * grad\n",
    "    theta_tilde = theta + eps * r_tilde\n",
    "    logp_tilde, grad_tilde = L(theta_tilde)\n",
    "    r_tilde = r_tilde + (eps/2) * grad_tilde\n",
    "    return theta_tilde, r_tilde, grad_tilde, logp_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindReasonableEpsilon(theta, grad, logp, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    From Algorithm 4 in NUTS.\n",
    "    Heuristic for choosing an initial value of epsilon.\n",
    "    Helper function for Algorithm 6.\n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    eps = value for initial value of epsilon, step size\n",
    "    \n",
    "    \"\"\"\n",
    "    #initialize\n",
    "    parems = len(theta)\n",
    "    eps = 1\n",
    "    r = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "    r = r.ravel()\n",
    "    theta_prime, r_prime, _, logp_prime = Leapfrog(theta, r, grad, eps, L)\n",
    "    \n",
    "    prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r, r)))\n",
    "    a = 2 * int(prob > 0.5) - 1\n",
    "\n",
    "    while prob**a > 2**(-a):\n",
    "        eps = 2**a * eps\n",
    "        theta_prime, r_prime, _, logp_prime = Leapfrog(theta, r, grad, eps, L);\n",
    "        prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r,r)))\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BuildTree(theta, r, grad, u, v, j, eps, L, esto0):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    builds tree in NUTS sampler, helper function for Algorithm 6\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    v = direction in creating tree, value from -1 to 1\n",
    "    \n",
    "    j = height of tree, starts at 0\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    r_theta0 = joint probability of theta0 and r\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if j == 0: \n",
    "        # base case, take one leapfrog step in the direction v\n",
    "        theta_prime, r_prime, grad_prime, logp_prime = Leapfrog(theta, r, grad, v*eps, L)\n",
    "        \n",
    "        esto = logp_prime - 0.5 * np.dot(r_prime, r_prime)\n",
    "\n",
    "        n_prime = int(u <= np.exp(esto))\n",
    "        \n",
    "        s_prime = int(esto > np.log(u) - 1000)\n",
    "\n",
    "        alpha_prime = min(1, np.exp(esto - esto0))\n",
    "                                                   \n",
    "        return theta_prime, r_prime, grad_prime, theta_prime, r_prime, grad_prime, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha_prime, 1\n",
    "    else:\n",
    "        # recursion, build left and right subtrees\n",
    "        theta_minus, r_minus, grad_minus, theta_plus, r_plus, grad_plus, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha_prime, n_alpha_prime = BuildTree(theta, r, grad, u, v, j-1, eps, L, esto0)\n",
    "        \n",
    "        if s_prime == 1:\n",
    "            if v == -1:\n",
    "                theta_minus, r_minus, grad_minus, _,_,_, theta_doub_prime, grad_doub_prime, logp_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree(theta_minus, r_minus, grad_minus, u, v, j-1, eps, L, esto0)\n",
    "            else:\n",
    "                _, _, _, theta_plus, r_plus, grad_plus, theta_doub_prime, grad_doub_prime, logp_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree(theta_plus, r_plus, grad_plus, u, v, j-1, eps, L, esto0)\n",
    "\n",
    "            # Use Metropolis-Hastings\n",
    "            prob = n_doub_prime / max(n_prime + n_doub_prime,1)\n",
    "            if (np.random.uniform(0, 1, 1) < prob):\n",
    "                theta_prime = theta_doub_prime\n",
    "                grad_prime = grad_doub_prime\n",
    "                logp_prime = logp_doub_prime\n",
    "            \n",
    "            ind_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            ind_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s_prime = s_prime * s_doub_prime * ind_1 * ind_2\n",
    "            n_prime = n_prime + n_doub_prime\n",
    "            alpha_prime = alpha_prime + alpha_doub_prime\n",
    "            n_alpha_prime = n_alpha_prime + n_alpha_doub_prime\n",
    "        \n",
    "        return theta_minus, r_minus, grad_minus, theta_plus, r_plus, grad_plus, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha_prime, n_alpha_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running examples (seen more in depth below), we calculated the speed up ratios for various examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table of examples of speed up ratios is shown below. The examples are further explained below. \n",
    "\n",
    "With 5000 iterations of the algorithm and a data set of 100 observations, the times and the speed up ratios are shown below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|            | Slow | Fast | Ratio |\n",
    "|------------|------|------|-------|\n",
    "| Poisson    |      |      |       |\n",
    "| MVN (10-d) |      |      |       |\n",
    "| MVN (50-d) |      |      |       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Poisson Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we tested the code with a simple Poisson example. We generated simulated data from a Poisson distribution, and then fit a Poisson model likelihood with an improper uniform prior. The mean of the sampled $\\theta's$ was very close to the mean of the data, so we considered this successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random poisson data\n",
    "X_pois = np.random.poisson(5, 100)\n",
    "def L_pois(theta):\n",
    "    grad = sum(X_pois) / theta - 100\n",
    "    logp = sum(X_pois)*np.log(theta) - theta*100\n",
    "    return logp, grad\n",
    "np.mean(X_pois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# run on poisson data\n",
    "samples_pois, burned_in_pois = nuts6_dual_averaging(np.array([2]), 5000, 2000, L_pois)\n",
    "np.mean(burned_in_pois)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# run on poisson data\n",
    "samples_pois, burned_in_pois_slow = nuts6_dual_averaging_slow(np.array([2]), 5000, 2000, L_pois)\n",
    "np.mean(burned_in_pois)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Normal Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next example we did was a multivariate normal.  We generated simulated data from a MVN distribution, and then fit a MVN model likelihood with an improper uniform prior. Once again, the mean of the sampled $\\theta's$ was very close to the mean of the data, so we considered this successful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVN 10 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# multivariate normal data\n",
    "X_MVN10 = np.random.multivariate_normal(np.ones(10), np.identity(10), 100)\n",
    "\n",
    "def L_MVN10(theta):\n",
    "    cumsum = 0\n",
    "    grad = 0\n",
    "    for x in X_MVN10:\n",
    "        cumsum += ((x - theta).T @ np.linalg.inv(np.identity(10))) @ (x - theta)\n",
    "        grad += (x - theta).T @ np.linalg.inv(np.identity(10)) \n",
    "    logp = -0.5 * cumsum\n",
    "    return logp, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# run for MVN\n",
    "samples_MVN10, burned_in_MVN10 = nuts6_dual_averaging(np.zeros(10), 5000, 1000, L_MVN10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# run for normal\n",
    "samples, burned_in = nuts6_dual_averaging_slow(np.zeros(10), 5000, 1000, L_MVN10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MVN 50 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test speed up ratio for 50-d\n",
    "# 50 dimensional multivariate normal data\n",
    "X_MVN50 = np.random.multivariate_normal(np.ones(50), np.identity(50), 100)\n",
    "\n",
    "def L_MVN50(theta):\n",
    "    cumsum = 0\n",
    "    grad = 0\n",
    "    for x in X_MVN50:\n",
    "        cumsum += ((x - theta).T @ np.linalg.inv(np.identity(50))) @ (x - theta)\n",
    "        grad += (x - theta).T @ np.linalg.inv(np.identity(50)) \n",
    "    logp = -0.5 * cumsum\n",
    "    return logp, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "samples, burned_in = nuts6_dual_averaging(np.zeros(50), 5000, 1000, L_MVN50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "samples, burned_in = nuts6_dual_averaging_slow(np.zeros(50), 5000, 1000, L_MVN50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with other MCMC Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subset just accepted theta's\n",
    "unique_samples = np.array(samples_MVN10[0,:])\n",
    "for i in range(1, M):\n",
    "    if all(samples_MVN10[i-1,:] != samples_MVN10[i,:]):\n",
    "        unique_samples = np.vstack([unique_samples, samples_MVN10[i,:]])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(samples_MVN[:,0], samples_MVN[:,2], \".\")\n",
    "plt.plot(unique_samples[0:50,0], unique_samples[0:50,2], 'r')\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import numpy.random as rng\n",
    "import scipy.stats as stats\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "niter = 5000\n",
    "with pm.Model() as test_context:\n",
    "    mu = pm.MvNormal('mu', 0, tau=cov, shape= 10)\n",
    "    y = pm.MvNormal('y', mu=mu, tau= cov, observed=X_MVN10)\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(niter,step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metropolis_samples = trace['mu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_samples_met = np.array([metropolis_samples[0,:]])\n",
    "# subset just accepted theta's\n",
    "for i in range(1, niter):\n",
    "    if all(metropolis_samples[i-1,:] != metropolis_samples[i,:]):\n",
    "        unique_samples_met = np.vstack([unique_samples_met, metropolis_samples[i,:]])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(metropolis_samples[:,0], metropolis_samples[:,2], \".\")\n",
    "plt.plot(unique_samples_met[0:50,0], unique_samples_met[0:50,2], 'r')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUTS with dual averaging makes it possible for Bayesian data analysts to obtain the efficiency of HMC without spending time and effort hand-tuning HMC's parameters, making it possible to efficiently perform Bayesian posterior inference on a large class of complex, high-dimesional models with minimal human intervention.  \n",
    "\n",
    "We implemented the NUTS sampler in Python and were able to optimize our implementation of the NUTS sampler by removing the need to re-calculate the gradient and the log likelihood. We ran the NUTS sampler on various examples and compared them to another MCMC algorithm, Metropolis Hastings. \n",
    "\n",
    "The NUTS sampler appears to be overly complicated for simple examples, such as the ones we chose. However, it can very useful and efficient for higher order dimensional Bayesian problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. M. Hofmman and A. Gelman, \"The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo\", Journal of Machine Learning Research 15 (2014) 1351-1381\n",
    "\n",
    "2. A. Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari and D. Rubin, \"Bayesian Data Analysis\", Third Edition, CRC Press, 2014\n",
    "\n",
    "3. S. Chibb and E. Greenberg, \"Understanding the Metropolis-Hastings Algorithm\", The American Statistician, November 1995, Vol. 49, No. 4\n",
    "\n",
    "4. C. Robert and G. Casella, \"A Short History of Markov Chain Monte Carlo: Subjective Recollections from Incomplete Data\", Statistical Science, 2011, Vol. 26, No. 1, 102-115\n",
    "\n",
    "5. C. Andrieu and J. Thoms, \"A tutorial on adaptive MCMC\", Stat Comput (2008) 18: 343–373"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
