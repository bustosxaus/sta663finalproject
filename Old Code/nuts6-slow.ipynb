{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Leapfrog_slow(theta, r, eps, L):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    From Algorithm 1 in NUTS, Leapfrog step in Hamiltonian Monte Carlo/NUTS.\n",
    "    Helper function for Algorithm 6. \n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "       \n",
    "    OUTPUTS:\n",
    "    theta_tilde = update theta vector\n",
    "    \n",
    "    r_tilde = updated r momenta vector\n",
    "    \"\"\"\n",
    "    \n",
    "    logp, grad = L(theta)\n",
    "    r_tilde = r + (eps/2) * grad\n",
    "    theta_tilde = theta + eps * r_tilde\n",
    "    logp_tilde, grad_tilde = L(theta_tilde)\n",
    "    r_tilde = r_tilde + (eps/2) * grad_tilde\n",
    "    return theta_tilde, r_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindReasonableEpsilon_slow(theta, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    From Algorithm 4 in NUTS.\n",
    "    Heuristic for choosing an initial value of epsilon.\n",
    "    Helper function for Algorithm 6.\n",
    "    \n",
    "    INPUTS:\n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    eps = value for initial value of epsilon, step size\n",
    "    \n",
    "    \"\"\"\n",
    "    #initialize\n",
    "    parems = len(theta)\n",
    "    eps = 1\n",
    "    r = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "    r = r.ravel()\n",
    "    theta_prime, r_prime = Leapfrog_slow(theta, r, eps, L)\n",
    "    \n",
    "    logp, grad = L(theta)\n",
    "    logp_prime, grad_prime = L(theta_prime)\n",
    "    \n",
    "    prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r, r)))\n",
    "    a = 2 * int(prob > 0.5) - 1\n",
    "\n",
    "    while prob**a > 2**(-a):\n",
    "        eps = 2**a * eps\n",
    "        theta_prime, r_prime = Leapfrog_slow(theta, r, eps, L);\n",
    "        logp_prime, grad_prime = L(theta_prime)\n",
    "        prob = np.exp(logp_prime - logp - 0.5 * (np.dot(r_prime, r_prime) - np.dot(r,r)))\n",
    "    return eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def BuildTree_slow(theta, r, u, v, j, eps, r_theta0, L):\n",
    "    \"\"\"\n",
    "    SUMMARY:\n",
    "    builds tree in NUTS sampler, helper function for Algorithm 6\n",
    "    \n",
    "    INPUTS:\n",
    "    \n",
    "    theta = vector of parameters, len(theta) = number of parameters in the model, \n",
    "    np.array or list\n",
    "    \n",
    "    r = input from other function, momentum in Hamiltonian dynamics\n",
    "    \n",
    "    v = direction in creating tree, value from -1 to 1\n",
    "    \n",
    "    j = height of tree, starts at 0\n",
    "    \n",
    "    eps = step size\n",
    "    \n",
    "    r_theta0 = joint probability of theta0 and r\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime\n",
    "    \n",
    "    \"\"\"\n",
    "    if j == 0: \n",
    "        # base case, take one leapfrog step in the direction v\n",
    "        theta_prime, r_prime = Leapfrog_slow(theta, r, v*eps, L)\n",
    "        logp_prime, grad_prime = L(theta_prime)\n",
    "        logp_0, grad0 = L(theta0)\n",
    "        \n",
    "        r_theta = logp_prime - 0.5 * np.dot(r_prime, r_prime)      \n",
    "\n",
    "        n_prime = int(u <= np.exp(r_theta))   \n",
    "        s_prime = int(r_theta > np.log(u) - 1000)\n",
    "        alpha_prime = min(1, np.exp(r_theta - r_theta0))\n",
    "                                                   \n",
    "        return theta_prime, r_prime, theta_prime, r_prime, theta_prime, n_prime, s_prime, alpha_prime, 1\n",
    "    else:\n",
    "        # recursion, build left and right subtrees\n",
    "        theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime = BuildTree_slow(theta, r, u, v, j-1, eps, r_theta0, L)\n",
    "        \n",
    "        if s_prime == 1:\n",
    "            if v == -1:\n",
    "                theta_minus, r_minus, _,_, theta_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree_slow(theta_minus, r_minus, u, v, j-1, eps, r_theta0, L)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_doub_prime, n_doub_prime, s_doub_prime, alpha_doub_prime, n_alpha_doub_prime = BuildTree_slow(theta_plus, r_plus, u, v, j-1, eps, r_theta0, L)\n",
    "\n",
    "            # Use Metropolis-Hastings\n",
    "            prob = n_doub_prime / max(n_prime + n_doub_prime,1)\n",
    "            if (np.random.uniform(0, 1, 1) < prob):\n",
    "                theta_prime = theta_doub_prime\n",
    "            \n",
    "            ind_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            ind_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s_prime = s_prime * s_doub_prime * ind_1 * ind_2\n",
    "            n_prime = n_prime + n_doub_prime\n",
    "            alpha_prime = alpha_prime + alpha_doub_prime\n",
    "            n_alpha_prime = n_alpha_prime + n_alpha_doub_prime\n",
    "        \n",
    "        return theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha_prime, n_alpha_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nuts6_dual_averaging_slow(theta0, M, M_adapt, L, delta = 0.6):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    Not optimized version. \n",
    "    Implemented from Algorithm 6: NUTS with Dual Averaging.\n",
    "    Needs helper functions BuildTree_slow, FindReasonableEpsilon_slow, and Leapfrog_slow.\n",
    "    \n",
    "\n",
    "    INPUTS:\n",
    "    \n",
    "    theta0 = initial values for values of parameters in model. len(theta0) = number of parameters\n",
    "    \n",
    "    M = number of samples desired\n",
    "    \n",
    "    M_adapt = the number of steps for the burn-in,\n",
    "    also how long to run the dual averaging algorithm to find the appropriate epsilon\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    delta = target HMC acceptance probability.\n",
    "    default value of 0.6\n",
    "    is a value between 0 and 1\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    samples = np.array matrix of samples of theta from algorithm\n",
    "    dimensions of matrix are M x len(theta0)\n",
    "    \n",
    "    burned_in = same as samples matrix with burn_in samples removed\n",
    "    dimensions of matrix are are M-M_adapt x len(theta0)\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize samples matrix\n",
    "    # put initial theta0 in first row of matrix\n",
    "    parems = len(theta0)\n",
    "    samples = np.empty((M+1, parems))\n",
    "    samples[0, :] = theta0\n",
    "    eps = FindReasonableEpsilon_slow(theta0, L)\n",
    "    mu = np.log(10*eps)\n",
    "    eps_bar = 1\n",
    "    H_bar = 0\n",
    "    gamma = 0.05\n",
    "    t0 = 10\n",
    "    k = 0.75\n",
    "    \n",
    "    for m in range(1, M+1):\n",
    "        # resample\n",
    "        norm_samp = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "        r0 = norm_samp.ravel()\n",
    "        logp, grad = L(samples[m-1,:])\n",
    "\n",
    "        r_theta = logp - 0.5 * np.dot(r0,r0)\n",
    "        # resample u ~ uniform([0, exp(inside)])\n",
    "        u = np.random.uniform(0, np.exp(r_theta), 1)\n",
    "\n",
    "        # initialize minus's and plus's\n",
    "        theta_minus = samples[m-1, :]\n",
    "        theta_plus = samples[m-1, :]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j = 0 \n",
    "        \n",
    "        samples[m, :] = samples[m-1, :]\n",
    "        n = 1\n",
    "        s = 1\n",
    "        \n",
    "        while s == 1:\n",
    "            v_j = np.random.uniform(-1,1,1)\n",
    "            if v_j == -1:\n",
    "                theta_minus, r_minus, _, _, theta_prime, n_prime, s_prime, alpha, n_alpha = BuildTree_slow(theta_minus, r_minus, u, v_j, j, eps, r_theta, L)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_prime, n_prime, s_prime, alpha, n_alpha = BuildTree_slow(theta_plus, r_plus, u, v_j, j, eps, r_theta, L)\n",
    "            \n",
    "            if s_prime == 1:\n",
    "                # Use Metropolis-Hastings\n",
    "                prob = min(1, n_prime/n)\n",
    "                if (np.random.uniform(0,1,1) < prob):\n",
    "                    samples[m, :] = theta_prime\n",
    "                    \n",
    "            n = n + n_prime\n",
    "\n",
    "            boolean_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            boolean_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s = s_prime * boolean_1 * boolean_2\n",
    "            j = j + 1\n",
    "            \n",
    "        if m <= M_adapt:\n",
    "            H_bar = (1 - 1/(m+t0))*H_bar + (1/(m+t0)) * (delta - alpha/n_alpha)\n",
    "            eps = np.exp(mu - np.sqrt(m)/gamma * H_bar)\n",
    "            eps_bar = np.exp(m**(-k) * np.log(eps) + (1-m**(-k))*np.log(eps_bar))\n",
    "        else:\n",
    "            eps = eps_bar\n",
    "            \n",
    "    burned_in = samples[M_adapt+1:M+1, :]\n",
    "    \n",
    "    return samples, burned_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nuts6_dual_averaging(theta0, M, M_adapt, L, delta = 0.6):\n",
    "    \"\"\"\n",
    "    SUMMARY: \n",
    "    Implemented from Algorithm 6: NUTS with Dual Averaging.\n",
    "    Needs helper functions BuildTree, FindReasonableEpsilon, and Leapfrog.\n",
    "    \n",
    "\n",
    "    INPUTS:\n",
    "    \n",
    "    theta0 = initial values for values of parameters in model. len(theta0) = number of parameters\n",
    "    \n",
    "    M = number of samples desired\n",
    "    \n",
    "    M_adapt = the number of steps for the burn-in,\n",
    "    also how long to run the dual averaging algorithm to find the appropriate epsilon\n",
    "    \n",
    "    L = function from model that returns the log likelihood and the gradient like:\n",
    "    logp, grad = L(theta)\n",
    "    \n",
    "    delta = target HMC acceptance probability.\n",
    "    default value of 0.6\n",
    "    is a value between 0 and 1\n",
    "    \n",
    "    \n",
    "    OUTPUTS:\n",
    "    \n",
    "    samples = np.array matrix of samples of theta from algorithm\n",
    "    dimensions of matrix are M x len(theta0)\n",
    "    \n",
    "    burned_in = same as samples matrix with burn_in samples removed\n",
    "    dimensions of matrix are are M-M_adapt x len(theta0)\n",
    "    \n",
    "    \"\"\"\n",
    "    # initialize samples matrix\n",
    "    # put initial theta0 in first row of matrix\n",
    "    parems = len(theta0)\n",
    "    samples = np.empty((M+1, parems))\n",
    "    samples[0, :] = theta0\n",
    "    logp, grad = L(theta0)\n",
    "    eps = FindReasonableEpsilon(theta0, grad, logp, L)\n",
    "    mu = np.log(10*eps)\n",
    "    eps_bar = 1\n",
    "    H_bar = 0\n",
    "    gamma = 0.05\n",
    "    t0 = 10\n",
    "    k = 0.75\n",
    "    \n",
    "    for m in range(1, M+1):\n",
    "        # resample\n",
    "        norm_samp = np.random.multivariate_normal(np.zeros(parems), np.identity(parems), 1)\n",
    "        r0 = norm_samp.ravel()\n",
    "\n",
    "        esto = logp - 0.5 * np.dot(r0,r0)\n",
    "        # resample u ~ uniform([0, exp(inside)])\n",
    "        u = np.random.uniform(0, np.exp(esto), 1)\n",
    "\n",
    "        # initialize minus's and plus's\n",
    "        theta_minus = samples[m-1, :]\n",
    "        theta_plus = samples[m-1, :]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j = 0\n",
    "        grad_minus = grad\n",
    "        grad_plus = grad\n",
    "        \n",
    "        j = 0\n",
    "        samples[m, :] = samples[m-1, :]\n",
    "        n = 1\n",
    "        s = 1\n",
    "        \n",
    "        while s == 1:\n",
    "            v_j = np.random.uniform(-1,1,1)\n",
    "            if v_j == -1:\n",
    "                theta_minus, r_minus, grad_minus, _, _, _, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha, n_alpha = BuildTree(theta_minus, r_minus, grad_minus, u, v_j, j, eps, L, esto)\n",
    "            else:\n",
    "                _, _, _, theta_plus, r_plus, grad_plus, theta_prime, grad_prime, logp_prime, n_prime, s_prime, alpha, n_alpha = BuildTree(theta_plus, r_plus, grad_plus, u, v_j, j, eps, L, esto)\n",
    "            \n",
    "            if s_prime == 1:\n",
    "                # Use Metropolis-Hastings\n",
    "                prob = min(1, n_prime/n)\n",
    "                if (np.random.uniform(0,1,1) < prob):\n",
    "                    samples[m, :] = theta_prime\n",
    "                    logp = logp_prime\n",
    "                    grad = grad_prime\n",
    "                    \n",
    "            n = n + n_prime\n",
    "\n",
    "            boolean_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            boolean_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s = s_prime * boolean_1 * boolean_2\n",
    "            j = j + 1\n",
    "        \n",
    "        eta = 1 / (m + t0)\n",
    "        H_bar = (1 - eta)*H_bar + eta * (delta - alpha/n_alpha) \n",
    "        if m <= M_adapt:  \n",
    "            eps = np.exp(mu - np.sqrt(m)/gamma * H_bar)\n",
    "            eta = m**(-k)\n",
    "            eps_bar = np.exp((1-eta) * np.log(eps_bar) + eta * np.log(eps))\n",
    "        else:\n",
    "            eps = eps_bar\n",
    "            \n",
    "    burned_in = samples[M_adapt+1:M+1, :]\n",
    "    return samples, burned_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 51s, sys: 793 ms, total: 2min 52s\n",
      "Wall time: 2min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# run for normal\n",
    "samples, burned_in = nuts6_dual_averaging_slow(np.zeros(10), 5000, 1000, L_MVN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
