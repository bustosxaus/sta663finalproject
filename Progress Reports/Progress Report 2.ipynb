{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STA 663 Final Project: “The No-U-Turn Sampler”  \n",
    "Progress Report 2  \n",
    "Sarah Normoyle, Gonzalo Bustos \n",
    "April 21, 2016  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headings: \n",
    "\n",
    "Abstract  \n",
    "Background \n",
    "Implementation  \n",
    "Testing  \n",
    "Optimization  \n",
    "Application with Data and Model  \n",
    "Comparison with Stan and other MCMC Algorithms  \n",
    "Conclusion  \n",
    "References  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Background:  \n",
    "\n",
    "For many models, Monte Carlo Markov Chain (MCMC) methods such as Gibbs sampling and the Metropolis Hasting algorithm may not be efficient and may require a long time to converge. By using steps that are evaluated from the first-order gradient of the log posterior, Hamiltonian Monte Carlo (HMC) is an efficient MCMC algorithm that does not use random walk behavior. This paper by Matthew D. Hoffman and Andrew Gelman introduces a new algorithm, called the No-U-Turn Sampler (NUTS) that is an extension of Hamiltonian Monte Carlo. Unlike HMC, NUTS does not require the specification of the parameter for the number of steps, L. In addition, the use of a dual averaging technique is extended from HMC to NUTS in order to avoid the specification of a step size parameter, $\\epsilon$. Therefore, unlike HMC, NUTS can be implemented without having to hand-tune both of the two parameters, L and $\\epsilon$. In our report, we will implement the Naive NUTS Algorithm and also extend to the NUTS Algorithm with Dual Averaging. We will compare the efficiency and the results of this algorithm to other MCMC algorithms in Stan when used for a specific model and data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Algorithms and Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Leapfrog step** from **Algorithm 1**\n",
    "\n",
    "**function** Leapfrog$(\\theta,r,\\varepsilon)$\n",
    "\n",
    "Set $\\tilde{r} \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta)$\n",
    "\n",
    "Set $\\tilde{\\theta} \\leftarrow \\theta + \\varepsilon \\tilde{r}$\n",
    "\n",
    "Set $\\tilde{r} \\leftarrow r + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\tilde{\\theta})$\n",
    "\n",
    "**return** $\\tilde{\\theta},\\tilde{r}$\n",
    "\n",
    "$\\mathcal{L}$ is the logarithm of the joint density of the variables of interest $\\theta$. The Leapfrog function of Algorithm 1 implements the Stormer-Verlet (\"leapfrog\") integrator, which proceeds according to the updates:\n",
    "\n",
    "$r^{t + \\frac{\\varepsilon}{2}} = r^t + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta^t)$\n",
    "\n",
    "$\\theta^{t + \\varepsilon} = \\theta^t + \\varepsilon r^{t + \\frac{\\varepsilon}{2}}$\n",
    "\n",
    "$r^{t + \\varepsilon} = r^{t + \\frac{\\varepsilon}{2}} + \\frac{\\varepsilon}{2} \\nabla_\\theta \\mathcal{L}(\\theta^{t + \\varepsilon})$\n",
    "\n",
    "where $r^t$ and $\\theta^t$ denote the values of the momentum and position variables $r$ and $\\theta$ at time $t$, $\\nabla_\\theta$ denotes the gradient with respect to $\\theta$ and $\\varepsilon$ is the step size parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of Hamiltonian Monte Carlo (HMC) depends strongly on choosing suitable values for $\\varepsilon$ and $L$, which is the number of times chosen to run the leapfrog step. If $\\varepsilon$ is too large, then the simulation will be inaccurate and yield low acceptance rates. If $\\varepsilon$ is too small, then computation will be wasted taking many small steps. If $L$ is too small, then successive samples will be close to one another, resulting in undesirable random walk behavior and slow mixing. If $L$ is too large, then HMC will generate trajectories that loop back and retrace their steps.\n",
    "\n",
    "The No-U-Turn Sampler (NUTS) is an extension of HMC that eliminates the need to specify a fixed value of $L$, the number of leapfrog steps. It also incorporates schemes for setting $\\varepsilon$ based on a dual averaging algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm 3** Efficient NUTS\n",
    "\n",
    "Given $\\theta^0, \\varepsilon, \\mathcal{L}, M$:\n",
    "\n",
    "**for** $m=1$ to $M$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $r^0 \\sim \\mathcal{N}(0,I)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Resample $u \\sim \\text{Uniform}([0, \\text{exp}\\{\\mathcal{L}(\\theta^{m-1}) - \\frac{1}{2} r^0 \\cdot r^0 \\}])$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize $\\theta^- = \\theta^{m-1},~\\theta^+ = \\theta^{m-1},~r^- = r^0,~r^+ = r^0,~j = 0,~\\theta^m=\\theta^{m-1},~n=1,~s=1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **while** $s=1$ **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Choose a direction $v_j \\sim \\text{Uniform}(\\{-1,1\\})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v_j=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v_j,j,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v_j,j,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\text{min}\\big\\{1,\\frac{n'}{n}\\big\\}$, set $\\theta^m \\leftarrow \\theta'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n \\leftarrow n + n'$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s \\leftarrow s' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $j \\leftarrow j+1$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end while**\n",
    "\n",
    "**end for**\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**function** BuildTree$(\\theta,r,u,v,j,\\varepsilon)$\n",
    "\n",
    "**if** $j=0$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Base case - take one leapfrog step in the direction $v$*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta',r' \\leftarrow \\text{Leapfrog}(\\theta,r,v\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow \\mathbb{1}[u \\leq \\text{exp}\\{\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' \\}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow \\mathbb{1}[\\mathcal{L}(\\theta') - \\frac{1}{2} r' \\cdot r' > \\text{log}~u - \\Delta_{max}]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta',r',\\theta',r',\\theta',n',s'$\n",
    "\n",
    "**else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; *Recursion - implicitly build the left and right subtrees*\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s' \\leftarrow \\text{BuildTree}(\\theta,r,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **if** $s'=1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **if** $v=-1$ **then**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\theta^-,r^-,-,-,\\theta'',n'',s'' \\leftarrow \\text{BuildTree}(\\theta^-,r^-,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **else**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $-,-,\\theta^+,r^+,\\theta'',n'',s'' \\leftarrow \\text{BuildTree}(\\theta^+,r^+,u,v,j-1,\\varepsilon)$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; With probability $\\frac{n''}{n'+n''}$, set $\\theta' \\leftarrow \\theta''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s' \\leftarrow s'' \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^- \\geq 0] \\mathbb{1}[(\\theta^+ - \\theta^-) \\cdot r^+ \\geq 0]$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $n' \\leftarrow n' + n''$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end if**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **return** $\\theta^-,r^-,\\theta^+,r^+,\\theta',n',s'$\n",
    "\n",
    "**end if**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before developing Algorithm 3, Efficient NUTS, the paper develops Algorithm 2, Naive NUTS. Algorithm 2 introduces a slice variable $u$ with conditional distribution $p(u|\\theta,r)=\\text{Uniform}(u;[0,\\text{exp}\\{\\mathcal{L}(\\theta) - \\frac{1}{2} r \\cdot r \\}])$, which renders the conditional distribution $p(\\theta,r|u) = \\text{Uniform} (\\theta,r;\\{\\theta',r'|\\mathcal{L}(\\theta) - \\frac{1}{2} r \\cdot r \\} \\geq u \\})$. After resampling $u|\\theta,r$, NUTS uses the leapfrog algorithm to trace out a path forwards and backwards, doing for 1 step, 2 steps, 4 steps, etc. This doubling process builds a balanced binary tree whose leaf-nodes correspond to position-momentum states. The process is halted when the trajectory starts to double back on itself. \n",
    "\n",
    "In summary, Algorithm 2 leaves the target distribution $p(\\theta) \\propto \\text{exp}\\{\\mathcal{L}(\\theta)\\}$ invariant. It achieves this by resampling the momentum and slice variables $r$ and $u$, simulating a Hamiltonian trajectory forwards and backwards in time until that trajectory either begins retracing its steps or encounters a state with very low probability, selecting a subset of the states encountered on that trajectory that lie within the slice defined by the slice variable $u$, and finally choosing the next position and momentum variables $\\theta^m$ and $r$ uniformly at random from the subset of the states encountered.\n",
    "\n",
    "Algorithm 3 improves Algorithm 2 by breaking out of the recursion as soon as a zero value for the stop indicator $s$ is encountered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Written so far:  \n",
    "Efficient NUTS sampler  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from Algorithm 1: HMC, page1353\n",
    "def Leapfrog(theta, r, eps):\n",
    "    r_tilde = r + (eps/2) * L_gradient(*theta)\n",
    "    theta_tilde = theta + eps * r_tilde\n",
    "    r_tide = r_tilde + (eps/2) * L_gradient(*theta_tilde)\n",
    "    return theta_tilde, r_tilde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from Algorithm 3: Efficeint NUTS pg.1364\n",
    "def BuildTree(theta, r, u, v, j, eps):\n",
    "    if j == 0:\n",
    "        # base case, take one leapfrog step in the direction v\n",
    "        theta_prime, r_prime = Leapfrog(theta, r, v*eps)\n",
    "        \n",
    "        n_prime = int(u <= np.exp(L(*theta_prime) - 0.5 * np.dot(r_prime, r_prime)))\n",
    "        \n",
    "        s_prime = int(L(*theta_prime) - 0.5 * np.dot(r_prime, r_prime) > np.log(u) - 1000)\n",
    "        \n",
    "        return theta_prime, r_prime, theta_prime, r_prime, theta_prime, n_prime, s_prime\n",
    "    else:\n",
    "        # recursion, build left and right subtrees\n",
    "        theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime = BuildTree(theta, r, u, v, j-1, eps)\n",
    "        \n",
    "        if s_prime == 1:\n",
    "            if v == -1:\n",
    "                theta_minus, r_minus, _,_, theta_doub_prime, n_doub_prime, s_doub_prime = BuildTree(theta_minus, r_minus, u, v, j-1, eps)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_doub_prime, n_doub_prime, s_doub_prime = BuildTree(theta_plus, r_plus, u, v, j-1, eps)\n",
    "\n",
    "            # Use Metropolis-Hastings\n",
    "            prob = n_doub_prime / (n_prime + n_doub_prime)\n",
    "            if (np.random.uniform(0,1,1) < prob):\n",
    "                theta_prime = theta_doub_prime\n",
    "            \n",
    "            ind_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            ind_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s_prime = s_doub_prime * ind_1 * ind_2\n",
    "            n_prime = n_prime + n_doub_prime\n",
    "        \n",
    "        return theta_minus, r_minus, theta_plus, r_plus, theta_prime, n_prime, s_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def efficient_nuts(theta0, eps, L, M, L_gradient):\n",
    "    # initialize samples matrix\n",
    "    # put initial theta0 in first row of matrix\n",
    "    samples = np.empty((M+1, len(theta0)))\n",
    "    samples[0,:] = theta0\n",
    "    \n",
    "    for m in range(1, M+1):\n",
    "        # resample\n",
    "        norm_samp = np.random.multivariate_normal(np.repeat(0, len(theta0)), np.identity(len(theta0)), 1)\n",
    "        r0 = norm_samp.ravel()\n",
    "        upper = np.exp(L(*samples[m-1,:]) - 0.5*np.dot(r0, r0))\n",
    "        u = np.random.uniform(0, upper, 1)\n",
    "        \n",
    "        # initialize\n",
    "        theta_minus = samples[m-1,:]\n",
    "        theta_plus = samples[m-1,:]\n",
    "        r_minus = r0\n",
    "        r_plus = r0\n",
    "        j = 0\n",
    "        samples[m,:] = samples[m-1,:]\n",
    "        n = 1\n",
    "        s = 1\n",
    "        \n",
    "        while s == 1:\n",
    "            v_j = np.random.uniform(-1,1,1)\n",
    "            if v_j == -1:\n",
    "                theta_minus, r_minus, _, _, theta_minus, n_minus, s_prime = BuildTree(theta_minus, r_minus, u, v_j, j, eps)\n",
    "            else:\n",
    "                _, _, theta_plus, r_plus, theta_prime, n_prime, s_prime = BuildTree(theta_plus, r_plus, u, v_j, j, eps)\n",
    "            \n",
    "            if s_prime == 1:\n",
    "                # Use Metropolis-Hastings\n",
    "                prob = min(1, n_prime/n)\n",
    "                if (np.random.uniform(0,1,1) < prob):\n",
    "                    samples[m,:] = theta_prime\n",
    "                    \n",
    "            n = n + n_prime\n",
    "\n",
    "            boolean_1 = int(np.dot(theta_plus-theta_minus, r_minus) >= 0)\n",
    "            boolean_2 = int(np.dot(theta_plus-theta_minus, r_plus) >= 0)\n",
    "            s = s_prime * boolean_1 * boolean_2\n",
    "            j = j + 1\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "X = np.random.normal(1, 5, size = 50)\n",
    "n = 100\n",
    "\n",
    "def L(mu, var):\n",
    "    log_likelihood = -n/2 * np.log(var) - n/2 * np.log(2*np.pi) - sum((X - mu)**2) / 2*var\n",
    "    return log_likelihood\n",
    "\n",
    "def L_gradient(mu, var):\n",
    "    one = sum(X-mu) / var\n",
    "    two = -n/2*var + sum((X-mu)**2)/2*var**2\n",
    "    return np.array([one, two])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.00000000e+00,   5.00000000e+00],\n",
       "       [  4.98719944e+00,   1.32125375e+01],\n",
       "       [  4.95468585e+00,   7.85522548e+02],\n",
       "       [  4.98595771e+00,   2.53360174e+06],\n",
       "       [  4.96806990e+00,   3.50717315e+12],\n",
       "       [  5.06259792e+00,   4.67088034e+48],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98],\n",
       "       [  5.05880907e+00,   1.02908548e+98]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simple example\n",
    "theta0 = np.array([5,5])\n",
    "eps = 0.1\n",
    "M = 20\n",
    "efficient_nuts(theta0, eps, L, M, L_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
